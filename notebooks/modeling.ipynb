{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this module, after finished data preparation (required `panel.pkl` file existence), we will fit the panel data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "\n",
    "import pathlib\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "    from linearmodels import PanelOLS, RandomEffects\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = pathlib.Path(\"..\")\n",
    "DATA_PATH = ROOT_PATH.joinpath(\"_data\")\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "panel = pd.read_pickle(DATA_PATH.joinpath(\"panel.pkl\").resolve())\n",
    "panel = panel.set_index([\"Store\", \"Date\"])\n",
    "\n",
    "y = panel.pop(\"Sales\")\n",
    "X = panel\n",
    "\n",
    "X.rename(columns={\"Open\": \"Constant\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results cross-validation\n",
    "\n",
    "For time series or panel data models cross-validation is sneaky.\n",
    "We can't just split the data into k-folds and then average the results, because then we would assume that past data can be modeled with future data.\n",
    "What we need to do is gradually add subsequent periods and validate on the first period following the most recent period taken into training.\n",
    "We have a large data set, hence we will only validate on two random days of each month, starting from the half of the given data set.\n",
    "It will give us enough comparison to achieve robust scores of estimator performance.\n",
    "\n",
    "State-of-the-art approach would require from us to build two models, one with fixed and second with random effects and make a Hausman test to decide which effect include in the estimator but now we will pick the safer approach and take the always **consistent estimator** (but maybe not the most effective one) and use the **FE** model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cross-validation score is: 0.434 with 0.381 std.\n"
     ]
    }
   ],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from futsal.utils import set_local_seed\n",
    "\n",
    "\n",
    "X_res = X.reset_index()\n",
    "y_res = y.reset_index()\n",
    "\n",
    "year_month_df = pd.DataFrame({\n",
    "    \"Year\": X_res[\"Date\"].dt.year,\n",
    "    \"Month\": X_res[\"Date\"].dt.month,\n",
    "    \"Day\": X_res[\"Date\"].dt.day\n",
    "}).drop_duplicates().sort_values(by=[\"Year\", \"Month\", \"Day\"])\n",
    "\n",
    "\n",
    "thresh_date = year_month_df.iloc[int(year_month_df.shape[0] / 2)]\n",
    "thresh_date = pd.Timestamp(\n",
    "    year=thresh_date[\"Year\"], month=thresh_date[\"Month\"], day=1\n",
    ")\n",
    "\n",
    "superior_date = year_month_df.iloc[-1]\n",
    "superior_date = pd.Timestamp(\n",
    "    year=superior_date[\"Year\"], month=superior_date[\"Month\"], day=superior_date[\"Day\"]\n",
    ")\n",
    "\n",
    "r2_scores = []\n",
    "with set_local_seed(SEED):\n",
    "    while thresh_date < superior_date:\n",
    "        X_train = X_res[X_res[\"Date\"] < thresh_date].set_index([\"Store\", \"Date\"])\n",
    "        y_train = y_res[y_res[\"Date\"] < thresh_date].set_index([\"Store\", \"Date\"])\n",
    "        X_test = X_res[X_res[\"Date\"] == thresh_date].set_index([\"Store\", \"Date\"])\n",
    "        y_test = y_res[y_res[\"Date\"] == thresh_date].set_index([\"Store\", \"Date\"])\n",
    "\n",
    "        days_add = np.random.randint(15, 30)\n",
    "        thresh_date += timedelta(days=days_add)\n",
    "        \n",
    "        fe_model = PanelOLS(\n",
    "            y_train, \n",
    "            X_train\n",
    "        )\n",
    "\n",
    "        fe_res = fe_model.fit(cov_type=\"robust\")\n",
    "        \n",
    "        y_pred = fe_model.predict(\n",
    "            fe_res.params,\n",
    "            exog=X_test[fe_res.params.index.tolist()]\n",
    "        )\n",
    "        r2_scores.append(r2_score(y_test, y_pred))\n",
    "\n",
    "print(\n",
    "    \"Average cross-validation score is: {0:.3f} with {1:.3f} std.\"\n",
    "    .format(\n",
    "        np.mean(r2_scores),\n",
    "        np.std(r2_scores)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see, as expected when predicting daily sales, which are characterized by bigger relative changes from one time period to another, that our predictions are burdened with a large standard deviation, while with a reasonably acceptable mean r2. That being sad we can finally fit our model on the full data set and interpret the coefficients assigned to the single variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          PanelOLS Estimation Summary                           \n",
      "================================================================================\n",
      "Dep. Variable:                  Sales   R-squared:                        0.5872\n",
      "Estimator:                   PanelOLS   R-squared (Between):              0.7401\n",
      "No. Observations:              843080   R-squared (Within):               0.3447\n",
      "Date:                Thu, Aug 08 2019   R-squared (Overall):              0.5872\n",
      "Time:                        05:59:52   Log-likelihood                -7.602e+06\n",
      "Cov. Estimator:                Robust                                           \n",
      "                                        F-statistic:                   3.747e+04\n",
      "Entities:                        1115   P-value                           0.0000\n",
      "Avg Obs:                       756.13   Distribution:               F(32,843047)\n",
      "Min Obs:                       590.00                                           \n",
      "Max Obs:                       940.00   F-statistic (robust):          2.444e+04\n",
      "                                        P-value                           0.0000\n",
      "Time periods:                     940   Distribution:               F(32,843047)\n",
      "Avg Obs:                       896.89                                           \n",
      "Min Obs:                       16.000                                           \n",
      "Max Obs:                       1115.0                                           \n",
      "                                                                                \n",
      "                                    Parameter Estimates                                    \n",
      "===========================================================================================\n",
      "                         Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
      "-------------------------------------------------------------------------------------------\n",
      "Constant                    7366.3     43.947     167.62     0.0000      7280.2      7452.5\n",
      "Promo                       2664.2     10.473     254.38     0.0000      2643.7      2684.7\n",
      "SchoolHoliday               314.83     15.721     20.025     0.0000      284.01      345.64\n",
      "CustomersYesterday          6.0402     0.0101     600.31     0.0000      6.0205      6.0600\n",
      "OpenYesterday              -4847.8     56.657    -85.564     0.0000     -4958.8     -4736.7\n",
      "PromoYesterday             -1387.0     9.5169    -145.74     0.0000     -1405.6     -1368.3\n",
      "SchoolHolidayYesterday     -468.11     10.757    -43.516     0.0000     -489.20     -447.03\n",
      "OpenTomorrow               -1230.6     42.344    -29.062     0.0000     -1313.6     -1147.6\n",
      "SchoolHolidayTomorrow       136.59     13.076     10.446     0.0000      110.96      162.22\n",
      "ChristmasInAWeek            1679.1     19.185     87.520     0.0000      1641.5      1716.7\n",
      "DayOfWeek_2                -455.19     56.389    -8.0724     0.0000     -565.71     -344.67\n",
      "DayOfWeek_3                -315.89     56.380    -5.6029     0.0000     -426.39     -205.39\n",
      "DayOfWeek_4                -150.48     56.348    -2.6706     0.0076     -260.92     -40.041\n",
      "DayOfWeek_5                 150.36     56.561     2.6584     0.0079      39.505      261.22\n",
      "DayOfWeek_6                -940.23     68.936    -13.639     0.0000     -1075.3     -805.12\n",
      "DayOfWeek_7                 761.51     96.637     7.8801     0.0000      572.10      950.91\n",
      "StateHoliday_a             -144.13     141.10    -1.0214     0.3071     -420.69      132.43\n",
      "StateHoliday_b              851.49     343.96     2.4756     0.0133      177.34      1525.6\n",
      "StateHoliday_c              7711.9     663.63     11.621     0.0000      6411.2      9012.6\n",
      "StateHolidayYesterday_a     970.68     58.268     16.659     0.0000      856.47      1084.9\n",
      "StateHolidayYesterday_b     1816.0     68.921     26.349     0.0000      1680.9      1951.1\n",
      "StateHolidayYesterday_c     187.35     85.078     2.2021     0.0277      20.603      354.10\n",
      "StateHolidayTomorrow_a     -301.42     44.014    -6.8483     0.0000     -387.69     -215.16\n",
      "StateHolidayTomorrow_b      319.62     60.244     5.3054     0.0000      201.54      437.69\n",
      "StateHolidayTomorrow_c     -7032.4     71.646    -98.155     0.0000     -7172.8     -6892.0\n",
      "CompetitionDistance         0.0067     0.0003     21.974     0.0000      0.0061      0.0073\n",
      "Promo2Active               -87.939     5.6245    -15.635     0.0000     -98.963     -76.915\n",
      "CompetitionActive          -141.67     8.1658    -17.350     0.0000     -157.68     -125.67\n",
      "StoreType_b                -1556.5     36.691    -42.421     0.0000     -1628.4     -1484.5\n",
      "StoreType_c                 18.044     6.4349     2.8041     0.0050      5.4320      30.656\n",
      "StoreType_d                 807.19     5.3188     151.76     0.0000      796.77      817.62\n",
      "Assortment_b               -3860.5     50.343    -76.684     0.0000     -3959.2     -3761.8\n",
      "Assortment_c                532.40     4.6228     115.17     0.0000      523.33      541.46\n",
      "===========================================================================================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fe_model = PanelOLS(\n",
    "    y, \n",
    "    X\n",
    ")\n",
    "fe_res = fe_model.fit(cov_type=\"robust\")\n",
    "print(fe_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusions:\n",
    "\n",
    "* `StateHoliday_a`: Ghe coefficient of the variable is not significantly different from zero - you might consider removing this variable.\n",
    "* Days that most encourage customers to make large purchases are Mondays, Fridays and Sundays - you can suspect that the group is divided among those walking at the beginning of the week, when there is a lack of assortment on home shelves or at the very end on Friday, while it is common to go on Sunday for a larger group.\n",
    "* People tend to shop more when the store is closed the next day or was closed the day before, hence the negative coefficients adjusted to the variables: `OpenTomorrow`, `OpenYesterday`.\n",
    "* Temporary `Promo` seems to have a larger impact on the sales level unexpectedly unlike the `Promo2Active` feature, which suggests that sendind discount cards discourages people from shopping. More data would be needed to explain this phenomenon, because it is possible that these cards, sent in January, activate only in February and this variable should also be placed in a lagged approach.\n",
    "* The number of customers the day before has a huge impact on sales. this may suggest some trends that should be considered over a longer period of time, e.g. by observing the average number of customers over the last 7 days or the dynamics of change today to the same day but week before."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "futsal-venv",
   "language": "python",
   "name": "futsal-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
